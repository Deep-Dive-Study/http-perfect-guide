# 9. 웹 로봇

크롤러 : HTML을 기어다니며 모든 문서를 끌어온다.

- 루트 집합 : 크롤러의 출발 집합
- 순환 피하기 : 루프에 빠지지 않도록 방문흔적을 남겨야 한다.
- 방문 흔적 : URL을 저장한다면 매우 무거워지기 때문에 여러 대안이 생겼다.
    - 트리와 해시테이블 : URL을 추적해 트리나 해시테이블에 저장한다.
    - 느슨한 존재 비트맵 : 존재 비트 배열과 같은 느슨한 자료 구조를 사용한다. 해당 값이 존재한다면 크롤링된 URL로 간주
    - 체크포인트 : 크롤러가 중단될 수 있기 때문에 디스크에 URL을 저장한다.
    - 파티셔닝 : 여러 크롤러를 사용할 때 특정 부분을 할당하여 책임을 지도록 한다.
- URL 정규화 : 다른 URL일지라도 같은 것을 가리킬 때가 있다. 이를 위해 정규화를 한다.
    - 포트번호가 없다면 80포트로 추가한다.
    - 모든 %xx 이스케이핑 문자를 변환한다.
    - #태그들을 제거한다.
- 링크 순환 : 심벌릭 링크의 경우 순환에 빠지게 된다.

### 로봇의 HTTP

- 로봇은 보통 HTTP/1.0 요청을 보낸다. → 요구사항이 적기 때문
- 요청헤더를 통해 사이트에게 로봇을 식별할 수 있게 하는 것이 좋다. (User-Agent, From, Accept, Referer)
- 요청에 Host 헤더를 포함하지 않으면, 한 서버에서 여러개를 운영하는 경우 정확한 크롤링이 불가능하다.
- 조건부 요청을 통해 변경되었을 때만 컨텐츠를 다운받는다.
- 응답을 다루기 위해 로봇은 상태 코드, 엔티티를 해석할 수 있어야 하며 웹 관리자는 로봇의 요청을 다루기 위한 전략을 세워야 한다.

### 로봇의 부적절한 동작

- 폭주 : 서버 과부하 야기
- 오래된 URL : 존재하지 않는 URL에 많은 요청할 경우 웹 서버의 처리능력 저하 야기
- 길고 잘못된 URL : 길고 의미없는 URL을 많이 요청할 경우 웹 서버의 처리능력 저하 야기
- 호기심 : 사적인 컨텐츠를 타고 들어가 사생활 치매 야기
- 동적 게이트웨이 접근 : 게이트웨이 애플리케이션 콘텐츠에 요청할 경우 서버의 처리비용 부담 야기

### 로봇 차단하기

- robots.txt에 로봇이 접근에 대한 정보가 담겨져 있다.
- 로봇은 서버에 요청하기 전에 먼저 robots.txt를 요청하여 권한을 확인한다.
    - 2XX이 반환되면 그에 따른 규칙을 지킨다.
    - 3XX이 반환되면 리소스가 나올때까지 리다이렉트한다.
    - 401, 403이 반환되면 접근이 완전히 제안되어있다고 가정한다.
    - 404가 반환되면 제약이 없다고 생각하고 행동한다.
    - 503이 반환되면 리소스 요청을 멈춘다.

차단 방법

- 0.0 : robots.txt
- 1.0 : Allow 지시자 지원
- 2.0 : 표준 정규식과 타이밍 정보 포함

HTML 태그를 사용해 로봇을 제어할 수 있다. (헤더에 위치)

- <META NAME=”ROBOTS” CONTENT=directive-list> : 로봇 차단 태그
- <META NAME=”ROBOTS” CONTENT=”NOINDEX”> : 페이지 무시 명령
- <META NAME=”ROBOTS” CONTENT=”NOFOLLOW”> : 링크된 페이지 크롤링 금지
- INDEX : 인덱싱 가능
- FOLLOW : 링크된 페이지 크롤링 가능
- NOARCHIVE : 로컬 사본 생성 금지

### 로봇 에티켓

- 로봇의 신원을 밝혀라
- 로봇의 행동에 주의를 기울여라
- 로봇의 대역폭 소비를 미리 알려라
- 로봇의 진행상황을 추적하고, 모니터링하라.

- URL을 필터링하여 필요한 리소스만 확인하라.
- 동적 URL을 피하라. 로봇이 게이트웨이를 크롤링하는 방법도 모를테니
- Accept관련 헤더로 이해 가능한 콘텐츠를 알려라.
- robots.txt의 제어에 따르라.
- 접근 회수를 확인하여 특정 사이트에 너무 자주 방문하지 말아라.

- 모든 응답 코드에 대해 준비되어 있어야 한다.
- URL을 정규화 하여 중복을 제거하라
- 순환과 함정을 감지하고 피하라.
- 블랙 리스트로 관리하라

### 검색엔진

웹 로봇이 가장 광범위하게 사용하는 것은 검색엔진이다.

웹 크롤러들은 검색엔진에게 웹에 존재하는 문서를 가져다 주어서 어떤 문서에 어떤 단어들이 존재하는지에 대한 색인을 생성할 수 있게 한다.

웹의 금격한 성장과 함께 검색엔진도 복잡해졌다.

검색엔진은 수십억의 웹페이지를 검색하기 위해 복잡한 크롤러를 사용하게 되었고, 병렬처리 하도록 변경되었다.

현대는 full text index에 대한 질의를 통해 빠른 검색을 할 수 있다.

색인을 통해 결과 페이지를 만드는데, 검색어가 많이 등장하거나, 밀접한 text를 반환하기 위한 알고리즘이 따로 있다.

검색을 할 때 자신의 페이지가 상위에 노출되기 위해 가짜 페이지를 만들거나, 알고리즘을 속이기 위한 단어를 포함하기 시작했다.
